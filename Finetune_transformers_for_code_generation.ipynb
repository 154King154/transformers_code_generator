{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Finetune transformers for code generation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RHDK81QqrET"
      },
      "source": [
        "# Finetune transformers on code examples\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u21B9194XWAL"
      },
      "source": [
        "Пример дообучения **декодеров** (на примере gpt-2-medium) и дообучения **энкодеров** с помощью masked language modeling (на примере bert). Обучающая выборка представлена тремя скриптами из исходников библиотеки pandas, чтобы можно было оперативно попробовать запустить в collab.\n",
        "\n",
        "Также здесь не используется ускорение deepspeed, дабы сейчас это для тестового примера не нужно и в collab это не особо актуально (одна маленькая gpu), но в далльнейшем это можно без проблем добавить."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eK10D3MSpYty"
      },
      "source": [
        "## Install enviroment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCeon3bEsRwr"
      },
      "source": [
        "y!pip3 install folium==0.2.1\n",
        "!pip3 install urllib3==1.25.4\n",
        "!pip3 install --no-cache-dir transformers\n",
        "!pip3 install tqdm\n",
        "!git clone https://github.com/NVIDIA/apex\n",
        "!pip3 install -v --no-cache-dir apex"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NitGcEKPsDQE"
      },
      "source": [
        "## Run finetuning\n",
        "Example for decoders (gpt2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rbbvBDe5k_c",
        "outputId": "332f460e-a4a9-43f1-b23a-75dd342487d2"
      },
      "source": [
        "!python 'pretrain_transformers.py' \\\n",
        "            --output_dir='models' \\\n",
        "            --model_type=gpt2 \\\n",
        "            --model_name_or_path=gpt2-medium \\\n",
        "            --do_train \\\n",
        "            --train_data_file=codes.txt \\\n",
        "            --per_gpu_train_batch_size 1 \\\n",
        "            --gradient_accumulation_steps 1 \\\n",
        "            --num_train_epochs 2 \\\n",
        "            --block_size 512 \\\n",
        "            --save_total_limit 5 \\\n",
        "            --overwrite_output_dir \n",
        "            #--do_eval \\\n",
        "            #--eval_data_file=valid.txt \\\n",
        "            #--fp16 \\\n",
        "            #--fp16_opt_level O2 \\"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-07 16:32:09.200692: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "04/07/2021 16:32:10 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "04/07/2021 16:32:11 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-medium-config.json from cache at /root/.cache/torch/transformers/98aa65385e18b0efd17acd8bf64dcdf21406bb0c99c801c2d3c9f6bfd1f48f29.250d6dc755ccb17d19c7c1a7677636683aa35f0f6cb5461b3c0587bc091551a0\n",
            "04/07/2021 16:32:11 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"_num_labels\": 2,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bad_words_ids\": null,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"finetuning_task\": null,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"n_special\": 0,\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": null,\n",
            "  \"predict_special_tokens\": true,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "04/07/2021 16:32:11 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-medium-config.json from cache at /root/.cache/torch/transformers/98aa65385e18b0efd17acd8bf64dcdf21406bb0c99c801c2d3c9f6bfd1f48f29.250d6dc755ccb17d19c7c1a7677636683aa35f0f6cb5461b3c0587bc091551a0\n",
            "04/07/2021 16:32:11 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"_num_labels\": 2,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bad_words_ids\": null,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"finetuning_task\": null,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"n_special\": 0,\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": null,\n",
            "  \"predict_special_tokens\": true,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "04/07/2021 16:32:12 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-medium-vocab.json from cache at /root/.cache/torch/transformers/f20f05d3ae37c4e3cd56764d48e566ea5adeba153dcee6eb82a18822c9c731ec.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
            "04/07/2021 16:32:12 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-medium-merges.txt from cache at /root/.cache/torch/transformers/6d882670c55563617571fe0c97df88626fb5033927b40fc18a8acf98dafd4946.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
            "04/07/2021 16:32:12 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-medium-pytorch_model.bin from cache at /root/.cache/torch/transformers/4b337a4f3b7d3e1518f799e238af607498c02938a3390152aaec7d4dabca5a02.8769029be4f66a5ae1055eefdd1d11621b901d510654266b8681719fff492d6e\n",
            "04/07/2021 16:32:24 - INFO - transformers.modeling_utils -   Weights of GPT2LMHeadModel not initialized from pretrained model: ['lm_head.weight']\n",
            "04/07/2021 16:32:27 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir=None, config_name=None, device=device(type='cuda'), do_eval=False, do_train=True, eval_all_checkpoints=False, eval_data_file=None, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=5e-05, line_by_line=False, local_rank=-1, logging_steps=500, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_name_or_path='gpt2-medium', model_type='gpt2', n_gpu=1, no_cuda=False, num_train_epochs=2.0, output_dir='models', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=4, per_gpu_train_batch_size=1, save_steps=500, save_total_limit=5, seed=42, server_ip='', server_port='', should_continue=False, tokenizer_name=None, train_data_file='codes.txt', warmup_steps=0, weight_decay=0.01)\n",
            "04/07/2021 16:32:27 - INFO - __main__ -   Creating features from dataset file at \n",
            "04/07/2021 16:32:27 - INFO - __main__ -   Saving features into cached file gpt2_cached_lm_512_codes.txt\n",
            "04/07/2021 16:32:27 - INFO - __main__ -   ***** Running training *****\n",
            "04/07/2021 16:32:27 - INFO - __main__ -     Num examples = 65\n",
            "04/07/2021 16:32:27 - INFO - __main__ -     Num Epochs = 2\n",
            "04/07/2021 16:32:27 - INFO - __main__ -     Instantaneous batch size per GPU = 1\n",
            "04/07/2021 16:32:27 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "04/07/2021 16:32:27 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
            "04/07/2021 16:32:27 - INFO - __main__ -     Total optimization steps = 130\n",
            "Epoch:   0% 0/2 [00:00<?, ?it/s]\n",
            "Iteration:   0% 0/65 [00:00<?, ?it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:155: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:1005.)\n",
            "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
            "\n",
            "Iteration:   2% 1/65 [00:00<00:59,  1.07it/s]\u001b[A\n",
            "Iteration:   3% 2/65 [00:01<00:50,  1.24it/s]\u001b[A\n",
            "Iteration:   5% 3/65 [00:01<00:44,  1.38it/s]\u001b[A\n",
            "Iteration:   6% 4/65 [00:02<00:40,  1.51it/s]\u001b[A\n",
            "Iteration:   8% 5/65 [00:03<00:37,  1.60it/s]\u001b[A\n",
            "Iteration:   9% 6/65 [00:03<00:35,  1.68it/s]\u001b[A\n",
            "Iteration:  11% 7/65 [00:04<00:33,  1.73it/s]\u001b[A\n",
            "Iteration:  12% 8/65 [00:04<00:32,  1.78it/s]\u001b[A\n",
            "Iteration:  14% 9/65 [00:05<00:30,  1.81it/s]\u001b[A\n",
            "Iteration:  15% 10/65 [00:05<00:30,  1.83it/s]\u001b[A\n",
            "Iteration:  17% 11/65 [00:06<00:29,  1.84it/s]\u001b[A\n",
            "Iteration:  18% 12/65 [00:06<00:28,  1.85it/s]\u001b[A\n",
            "Iteration:  20% 13/65 [00:07<00:27,  1.87it/s]\u001b[A\n",
            "Iteration:  22% 14/65 [00:07<00:27,  1.88it/s]\u001b[A\n",
            "Iteration:  23% 15/65 [00:08<00:26,  1.88it/s]\u001b[A\n",
            "Iteration:  25% 16/65 [00:08<00:25,  1.89it/s]\u001b[A\n",
            "Iteration:  26% 17/65 [00:09<00:25,  1.89it/s]\u001b[A\n",
            "Iteration:  28% 18/65 [00:09<00:24,  1.88it/s]\u001b[A\n",
            "Iteration:  29% 19/65 [00:10<00:24,  1.88it/s]\u001b[A\n",
            "Iteration:  31% 20/65 [00:10<00:23,  1.88it/s]\u001b[A\n",
            "Iteration:  32% 21/65 [00:11<00:23,  1.88it/s]\u001b[A\n",
            "Iteration:  34% 22/65 [00:12<00:22,  1.88it/s]\u001b[A\n",
            "Iteration:  35% 23/65 [00:12<00:22,  1.88it/s]\u001b[A\n",
            "Iteration:  37% 24/65 [00:13<00:21,  1.88it/s]\u001b[A\n",
            "Iteration:  38% 25/65 [00:13<00:21,  1.88it/s]\u001b[A\n",
            "Iteration:  40% 26/65 [00:14<00:20,  1.87it/s]\u001b[A\n",
            "Iteration:  42% 27/65 [00:14<00:20,  1.87it/s]\u001b[A\n",
            "Iteration:  43% 28/65 [00:15<00:19,  1.86it/s]\u001b[A\n",
            "Iteration:  45% 29/65 [00:15<00:19,  1.86it/s]\u001b[A\n",
            "Iteration:  46% 30/65 [00:16<00:18,  1.86it/s]\u001b[A\n",
            "Iteration:  48% 31/65 [00:16<00:18,  1.84it/s]\u001b[A\n",
            "Iteration:  49% 32/65 [00:17<00:17,  1.84it/s]\u001b[A\n",
            "Iteration:  51% 33/65 [00:17<00:17,  1.85it/s]\u001b[A\n",
            "Iteration:  52% 34/65 [00:18<00:16,  1.85it/s]\u001b[A\n",
            "Iteration:  54% 35/65 [00:19<00:16,  1.85it/s]\u001b[A\n",
            "Iteration:  55% 36/65 [00:19<00:15,  1.85it/s]\u001b[A\n",
            "Iteration:  57% 37/65 [00:20<00:15,  1.85it/s]\u001b[A\n",
            "Iteration:  58% 38/65 [00:20<00:14,  1.85it/s]\u001b[A\n",
            "Iteration:  60% 39/65 [00:21<00:14,  1.84it/s]\u001b[A\n",
            "Iteration:  62% 40/65 [00:21<00:13,  1.84it/s]\u001b[A\n",
            "Iteration:  63% 41/65 [00:22<00:13,  1.84it/s]\u001b[A\n",
            "Iteration:  65% 42/65 [00:22<00:12,  1.84it/s]\u001b[A\n",
            "Iteration:  66% 43/65 [00:23<00:11,  1.84it/s]\u001b[A\n",
            "Iteration:  68% 44/65 [00:23<00:11,  1.84it/s]\u001b[A\n",
            "Iteration:  69% 45/65 [00:24<00:10,  1.84it/s]\u001b[A\n",
            "Iteration:  71% 46/65 [00:25<00:10,  1.83it/s]\u001b[A\n",
            "Iteration:  72% 47/65 [00:25<00:09,  1.83it/s]\u001b[A\n",
            "Iteration:  74% 48/65 [00:26<00:09,  1.83it/s]\u001b[A\n",
            "Iteration:  75% 49/65 [00:26<00:08,  1.82it/s]\u001b[A\n",
            "Iteration:  77% 50/65 [00:27<00:08,  1.82it/s]\u001b[A\n",
            "Iteration:  78% 51/65 [00:27<00:07,  1.82it/s]\u001b[A\n",
            "Iteration:  80% 52/65 [00:28<00:07,  1.82it/s]\u001b[A\n",
            "Iteration:  82% 53/65 [00:28<00:06,  1.82it/s]\u001b[A\n",
            "Iteration:  83% 54/65 [00:29<00:06,  1.83it/s]\u001b[A\n",
            "Iteration:  85% 55/65 [00:29<00:05,  1.84it/s]\u001b[A\n",
            "Iteration:  86% 56/65 [00:30<00:04,  1.84it/s]\u001b[A\n",
            "Iteration:  88% 57/65 [00:31<00:04,  1.84it/s]\u001b[A\n",
            "Iteration:  89% 58/65 [00:31<00:03,  1.84it/s]\u001b[A\n",
            "Iteration:  91% 59/65 [00:32<00:03,  1.83it/s]\u001b[A\n",
            "Iteration:  92% 60/65 [00:32<00:02,  1.83it/s]\u001b[A\n",
            "Iteration:  94% 61/65 [00:33<00:02,  1.82it/s]\u001b[A\n",
            "Iteration:  95% 62/65 [00:33<00:01,  1.82it/s]\u001b[A\n",
            "Iteration:  97% 63/65 [00:34<00:01,  1.82it/s]\u001b[A\n",
            "Iteration:  98% 64/65 [00:34<00:00,  1.82it/s]\u001b[A\n",
            "Iteration: 100% 65/65 [00:35<00:00,  1.83it/s]\n",
            "Epoch:  50% 1/2 [00:35<00:35, 35.44s/it]\n",
            "Iteration:   0% 0/65 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   2% 1/65 [00:00<00:35,  1.82it/s]\u001b[A\n",
            "Iteration:   3% 2/65 [00:01<00:34,  1.82it/s]\u001b[A\n",
            "Iteration:   5% 3/65 [00:01<00:34,  1.82it/s]\u001b[A\n",
            "Iteration:   6% 4/65 [00:02<00:33,  1.82it/s]\u001b[A\n",
            "Iteration:   8% 5/65 [00:02<00:32,  1.82it/s]\u001b[A\n",
            "Iteration:   9% 6/65 [00:03<00:32,  1.82it/s]\u001b[A\n",
            "Iteration:  11% 7/65 [00:03<00:31,  1.82it/s]\u001b[A\n",
            "Iteration:  12% 8/65 [00:04<00:31,  1.82it/s]\u001b[A\n",
            "Iteration:  14% 9/65 [00:04<00:30,  1.81it/s]\u001b[A\n",
            "Iteration:  15% 10/65 [00:05<00:30,  1.81it/s]\u001b[A\n",
            "Iteration:  17% 11/65 [00:06<00:29,  1.81it/s]\u001b[A\n",
            "Iteration:  18% 12/65 [00:06<00:29,  1.81it/s]\u001b[A\n",
            "Iteration:  20% 13/65 [00:07<00:28,  1.80it/s]\u001b[A\n",
            "Iteration:  22% 14/65 [00:07<00:28,  1.80it/s]\u001b[A\n",
            "Iteration:  23% 15/65 [00:08<00:27,  1.80it/s]\u001b[A\n",
            "Iteration:  25% 16/65 [00:08<00:27,  1.80it/s]\u001b[A\n",
            "Iteration:  26% 17/65 [00:09<00:26,  1.81it/s]\u001b[A\n",
            "Iteration:  28% 18/65 [00:09<00:26,  1.80it/s]\u001b[A\n",
            "Iteration:  29% 19/65 [00:10<00:25,  1.81it/s]\u001b[A\n",
            "Iteration:  31% 20/65 [00:11<00:24,  1.81it/s]\u001b[A\n",
            "Iteration:  32% 21/65 [00:11<00:24,  1.81it/s]\u001b[A\n",
            "Iteration:  34% 22/65 [00:12<00:23,  1.81it/s]\u001b[A\n",
            "Iteration:  35% 23/65 [00:12<00:23,  1.80it/s]\u001b[A\n",
            "Iteration:  37% 24/65 [00:13<00:22,  1.80it/s]\u001b[A\n",
            "Iteration:  38% 25/65 [00:13<00:22,  1.80it/s]\u001b[A\n",
            "Iteration:  40% 26/65 [00:14<00:21,  1.80it/s]\u001b[A\n",
            "Iteration:  42% 27/65 [00:14<00:21,  1.80it/s]\u001b[A\n",
            "Iteration:  43% 28/65 [00:15<00:20,  1.80it/s]\u001b[A\n",
            "Iteration:  45% 29/65 [00:16<00:20,  1.79it/s]\u001b[A\n",
            "Iteration:  46% 30/65 [00:16<00:19,  1.79it/s]\u001b[A\n",
            "Iteration:  48% 31/65 [00:17<00:18,  1.79it/s]\u001b[A\n",
            "Iteration:  49% 32/65 [00:17<00:18,  1.79it/s]\u001b[A\n",
            "Iteration:  51% 33/65 [00:18<00:17,  1.79it/s]\u001b[A\n",
            "Iteration:  52% 34/65 [00:18<00:17,  1.78it/s]\u001b[A\n",
            "Iteration:  54% 35/65 [00:19<00:16,  1.78it/s]\u001b[A\n",
            "Iteration:  55% 36/65 [00:19<00:16,  1.78it/s]\u001b[A\n",
            "Iteration:  57% 37/65 [00:20<00:15,  1.78it/s]\u001b[A\n",
            "Iteration:  58% 38/65 [00:21<00:15,  1.77it/s]\u001b[A\n",
            "Iteration:  60% 39/65 [00:21<00:14,  1.78it/s]\u001b[A\n",
            "Iteration:  62% 40/65 [00:22<00:14,  1.77it/s]\u001b[A\n",
            "Iteration:  63% 41/65 [00:22<00:13,  1.77it/s]\u001b[A\n",
            "Iteration:  65% 42/65 [00:23<00:13,  1.76it/s]\u001b[A\n",
            "Iteration:  66% 43/65 [00:23<00:12,  1.77it/s]\u001b[A\n",
            "Iteration:  68% 44/65 [00:24<00:11,  1.76it/s]\u001b[A\n",
            "Iteration:  69% 45/65 [00:25<00:11,  1.77it/s]\u001b[A\n",
            "Iteration:  71% 46/65 [00:25<00:10,  1.76it/s]\u001b[A\n",
            "Iteration:  72% 47/65 [00:26<00:10,  1.76it/s]\u001b[A\n",
            "Iteration:  74% 48/65 [00:26<00:09,  1.76it/s]\u001b[A\n",
            "Iteration:  75% 49/65 [00:27<00:09,  1.76it/s]\u001b[A\n",
            "Iteration:  77% 50/65 [00:27<00:08,  1.75it/s]\u001b[A\n",
            "Iteration:  78% 51/65 [00:28<00:07,  1.76it/s]\u001b[A\n",
            "Iteration:  80% 52/65 [00:29<00:07,  1.75it/s]\u001b[A\n",
            "Iteration:  82% 53/65 [00:29<00:06,  1.75it/s]\u001b[A\n",
            "Iteration:  83% 54/65 [00:30<00:06,  1.75it/s]\u001b[A\n",
            "Iteration:  85% 55/65 [00:30<00:05,  1.75it/s]\u001b[A\n",
            "Iteration:  86% 56/65 [00:31<00:05,  1.74it/s]\u001b[A\n",
            "Iteration:  88% 57/65 [00:31<00:04,  1.74it/s]\u001b[A\n",
            "Iteration:  89% 58/65 [00:32<00:04,  1.74it/s]\u001b[A\n",
            "Iteration:  91% 59/65 [00:33<00:03,  1.74it/s]\u001b[A\n",
            "Iteration:  92% 60/65 [00:33<00:02,  1.73it/s]\u001b[A\n",
            "Iteration:  94% 61/65 [00:34<00:02,  1.73it/s]\u001b[A\n",
            "Iteration:  95% 62/65 [00:34<00:01,  1.73it/s]\u001b[A\n",
            "Iteration:  97% 63/65 [00:35<00:01,  1.73it/s]\u001b[A\n",
            "Iteration:  98% 64/65 [00:35<00:00,  1.73it/s]\u001b[A\n",
            "Iteration: 100% 65/65 [00:36<00:00,  1.78it/s]\n",
            "Epoch: 100% 2/2 [01:12<00:00, 36.01s/it]\n",
            "04/07/2021 16:33:39 - INFO - __main__ -    global_step = 130, average loss = 1.6574663373140188\n",
            "04/07/2021 16:33:39 - INFO - __main__ -   Saving model checkpoint to models\n",
            "04/07/2021 16:33:39 - INFO - transformers.configuration_utils -   Configuration saved in models/config.json\n",
            "04/07/2021 16:33:45 - INFO - transformers.modeling_utils -   Model weights saved in models/pytorch_model.bin\n",
            "04/07/2021 16:33:45 - INFO - transformers.configuration_utils -   loading configuration file models/config.json\n",
            "04/07/2021 16:33:45 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"_num_labels\": 2,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bad_words_ids\": null,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"finetuning_task\": null,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"n_special\": 0,\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": null,\n",
            "  \"predict_special_tokens\": true,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "04/07/2021 16:33:45 - INFO - transformers.modeling_utils -   loading weights file models/pytorch_model.bin\n",
            "04/07/2021 16:34:08 - INFO - transformers.configuration_utils -   loading configuration file models/config.json\n",
            "04/07/2021 16:34:08 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"_num_labels\": 2,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bad_words_ids\": null,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"finetuning_task\": null,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"n_special\": 0,\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": null,\n",
            "  \"predict_special_tokens\": true,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "04/07/2021 16:34:08 - INFO - transformers.tokenization_utils -   Model name 'models' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'models' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "04/07/2021 16:34:08 - INFO - transformers.tokenization_utils -   Didn't find file models/added_tokens.json. We won't load it.\n",
            "04/07/2021 16:34:08 - INFO - transformers.tokenization_utils -   loading file models/vocab.json\n",
            "04/07/2021 16:34:08 - INFO - transformers.tokenization_utils -   loading file models/merges.txt\n",
            "04/07/2021 16:34:08 - INFO - transformers.tokenization_utils -   loading file None\n",
            "04/07/2021 16:34:08 - INFO - transformers.tokenization_utils -   loading file models/special_tokens_map.json\n",
            "04/07/2021 16:34:08 - INFO - transformers.tokenization_utils -   loading file models/tokenizer_config.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRlAAsIbsHdf",
        "outputId": "d0021365-0ba3-4213-b4f8-352f89d4d8bb"
      },
      "source": [
        "!python3 'generate_transformers.py' \\\n",
        "    --model_type=gpt2 \\\n",
        "    --model_name_or_path=models \\\n",
        "    --k=5 \\\n",
        "    --p=0.95 \\\n",
        "    --length=30"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-07 16:48:11.562710: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "04/07/2021 16:48:13 - INFO - transformers.tokenization_utils -   Model name 'models' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'models' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "04/07/2021 16:48:13 - INFO - transformers.tokenization_utils -   Didn't find file models/added_tokens.json. We won't load it.\n",
            "04/07/2021 16:48:13 - INFO - transformers.tokenization_utils -   loading file models/vocab.json\n",
            "04/07/2021 16:48:13 - INFO - transformers.tokenization_utils -   loading file models/merges.txt\n",
            "04/07/2021 16:48:13 - INFO - transformers.tokenization_utils -   loading file None\n",
            "04/07/2021 16:48:13 - INFO - transformers.tokenization_utils -   loading file models/special_tokens_map.json\n",
            "04/07/2021 16:48:13 - INFO - transformers.tokenization_utils -   loading file models/tokenizer_config.json\n",
            "04/07/2021 16:48:13 - INFO - transformers.configuration_utils -   loading configuration file models/config.json\n",
            "04/07/2021 16:48:13 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"_num_labels\": 2,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bad_words_ids\": null,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"finetuning_task\": null,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"n_special\": 0,\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": null,\n",
            "  \"predict_special_tokens\": true,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "04/07/2021 16:48:13 - INFO - transformers.modeling_utils -   loading weights file models/pytorch_model.bin\n",
            "04/07/2021 16:48:28 - INFO - __main__ -   Namespace(device=device(type='cuda'), k=5, length=30, model_name_or_path='models', model_type='gpt2', n_gpu=1, no_cuda=False, num_return_sequences=1, p=0.95, padding_text='', prompt='', repetition_penalty=1.0, seed=42, stop_token='</s>', temperature=1.0, xlm_language='')\n",
            "Context >>> import pandas as pd\n",
            "04/07/2021 16:49:07 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
            "ruGPT:\n",
            "import pandas as pd\n",
            "            from pandas import DataFrame, OrderedDict\n",
            "     \n",
            "Context >>> data = \n",
            "04/07/2021 16:49:19 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
            "ruGPT:\n",
            "data =  np.asarray(values, bins=False)\n",
            "    # TODO: Add support for pandas arrays\n",
            "    \n",
            "Context >>> np.\n",
            "04/07/2021 16:49:32 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
            "ruGPT:\n",
            "np.min)\n",
            "\n",
            "          # If the value is negative we are in the min range\n",
            "\n",
            "    \n",
            "Context >>> values = pd.\n",
            "04/07/2021 16:49:53 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
            "ruGPT:\n",
            "values = pd.get_dtype(values)\n",
            "\n",
            "      values, columns = values.sort() if columns.startswith(\"name\"\n",
            "Context >>> ^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6MFv1G1P1hC"
      },
      "source": [
        "Example for encoders (bert)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlC4lmuIO6ZM",
        "outputId": "11e334c3-c946-4a1e-eff8-f2e28ce9a578"
      },
      "source": [
        "!python 'pretrain_transformers.py' \\\n",
        "            --output_dir='models' \\\n",
        "            --mlm \\\n",
        "            --model_type=bert \\\n",
        "            --model_name_or_path=bert-base-cased\\\n",
        "            --do_train \\\n",
        "            --train_data_file=codes.txt \\\n",
        "            --per_gpu_train_batch_size 1 \\\n",
        "            --gradient_accumulation_steps 1 \\\n",
        "            --num_train_epochs 2 \\\n",
        "            --block_size 512 \\\n",
        "            --save_total_limit 5 \\\n",
        "            --overwrite_output_dir \n",
        "            #--do_eval \\\n",
        "            #--eval_data_file=valid.txt \\\n",
        "            #--fp16 \\\n",
        "            #--fp16_opt_level O2 \\"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-07 16:54:22.331744: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "04/07/2021 16:54:24 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "04/07/2021 16:54:24 - INFO - filelock -   Lock 140576476516304 acquired on /root/.cache/torch/transformers/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391.lock\n",
            "04/07/2021 16:54:24 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpnamwfb_6\n",
            "Downloading: 100% 433/433 [00:00<00:00, 375kB/s]\n",
            "04/07/2021 16:54:24 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json in cache at /root/.cache/torch/transformers/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391\n",
            "04/07/2021 16:54:24 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391\n",
            "04/07/2021 16:54:24 - INFO - filelock -   Lock 140576476516304 released on /root/.cache/torch/transformers/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391.lock\n",
            "04/07/2021 16:54:24 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at /root/.cache/torch/transformers/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391\n",
            "04/07/2021 16:54:24 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
            "  \"_num_labels\": 2,\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bad_words_ids\": null,\n",
            "  \"bos_token_id\": null,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": null,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "04/07/2021 16:54:25 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at /root/.cache/torch/transformers/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391\n",
            "04/07/2021 16:54:25 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
            "  \"_num_labels\": 2,\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bad_words_ids\": null,\n",
            "  \"bos_token_id\": null,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": null,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "04/07/2021 16:54:25 - INFO - filelock -   Lock 140576475647376 acquired on /root/.cache/torch/transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1.lock\n",
            "04/07/2021 16:54:25 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmp3n9275wt\n",
            "Downloading: 100% 213k/213k [00:00<00:00, 1.10MB/s]\n",
            "04/07/2021 16:54:25 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt in cache at /root/.cache/torch/transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n",
            "04/07/2021 16:54:25 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n",
            "04/07/2021 16:54:25 - INFO - filelock -   Lock 140576475647376 released on /root/.cache/torch/transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1.lock\n",
            "04/07/2021 16:54:25 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /root/.cache/torch/transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n",
            "04/07/2021 16:54:26 - INFO - filelock -   Lock 140576475662672 acquired on /root/.cache/torch/transformers/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2.lock\n",
            "04/07/2021 16:54:26 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpfjlem_qo\n",
            "Downloading: 100% 436M/436M [00:12<00:00, 36.2MB/s]\n",
            "04/07/2021 16:54:38 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin in cache at /root/.cache/torch/transformers/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2\n",
            "04/07/2021 16:54:38 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2\n",
            "04/07/2021 16:54:38 - INFO - filelock -   Lock 140576475662672 released on /root/.cache/torch/transformers/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2.lock\n",
            "04/07/2021 16:54:38 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin from cache at /root/.cache/torch/transformers/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2\n",
            "04/07/2021 16:54:42 - INFO - transformers.modeling_utils -   Weights of BertForMaskedLM not initialized from pretrained model: ['cls.predictions.decoder.bias']\n",
            "04/07/2021 16:54:42 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "04/07/2021 16:54:45 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir=None, config_name=None, device=device(type='cuda'), do_eval=False, do_train=True, eval_all_checkpoints=False, eval_data_file=None, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=5e-05, line_by_line=False, local_rank=-1, logging_steps=500, max_grad_norm=1.0, max_steps=-1, mlm=True, mlm_probability=0.15, model_name_or_path='bert-base-cased', model_type='bert', n_gpu=1, no_cuda=False, num_train_epochs=2.0, output_dir='models', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=4, per_gpu_train_batch_size=1, save_steps=500, save_total_limit=5, seed=42, server_ip='', server_port='', should_continue=False, tokenizer_name=None, train_data_file='codes.txt', warmup_steps=0, weight_decay=0.01)\n",
            "04/07/2021 16:54:45 - INFO - __main__ -   Creating features from dataset file at \n",
            "04/07/2021 16:54:45 - INFO - __main__ -   Saving features into cached file bert_cached_lm_510_codes.txt\n",
            "04/07/2021 16:54:45 - INFO - __main__ -   ***** Running training *****\n",
            "04/07/2021 16:54:45 - INFO - __main__ -     Num examples = 49\n",
            "04/07/2021 16:54:45 - INFO - __main__ -     Num Epochs = 2\n",
            "04/07/2021 16:54:45 - INFO - __main__ -     Instantaneous batch size per GPU = 1\n",
            "04/07/2021 16:54:45 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "04/07/2021 16:54:45 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
            "04/07/2021 16:54:45 - INFO - __main__ -     Total optimization steps = 98\n",
            "Epoch:   0% 0/2 [00:00<?, ?it/s]\n",
            "Iteration:   0% 0/49 [00:00<?, ?it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:155: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:1005.)\n",
            "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
            "\n",
            "Iteration:   2% 1/49 [00:00<00:14,  3.42it/s]\u001b[A\n",
            "Iteration:   4% 2/49 [00:00<00:11,  3.95it/s]\u001b[A\n",
            "Iteration:   6% 3/49 [00:00<00:10,  4.43it/s]\u001b[A\n",
            "Iteration:   8% 4/49 [00:00<00:09,  4.83it/s]\u001b[A\n",
            "Iteration:  10% 5/49 [00:00<00:08,  5.15it/s]\u001b[A\n",
            "Iteration:  12% 6/49 [00:01<00:07,  5.43it/s]\u001b[A\n",
            "Iteration:  14% 7/49 [00:01<00:07,  5.62it/s]\u001b[A\n",
            "Iteration:  16% 8/49 [00:01<00:07,  5.77it/s]\u001b[A\n",
            "Iteration:  18% 9/49 [00:01<00:06,  5.87it/s]\u001b[A\n",
            "Iteration:  20% 10/49 [00:01<00:06,  5.94it/s]\u001b[A\n",
            "Iteration:  22% 11/49 [00:01<00:06,  5.99it/s]\u001b[A\n",
            "Iteration:  24% 12/49 [00:02<00:06,  6.04it/s]\u001b[A\n",
            "Iteration:  27% 13/49 [00:02<00:05,  6.06it/s]\u001b[A\n",
            "Iteration:  29% 14/49 [00:02<00:05,  6.09it/s]\u001b[A\n",
            "Iteration:  31% 15/49 [00:02<00:05,  6.11it/s]\u001b[A\n",
            "Iteration:  33% 16/49 [00:02<00:05,  6.12it/s]\u001b[A\n",
            "Iteration:  35% 17/49 [00:02<00:05,  6.12it/s]\u001b[A\n",
            "Iteration:  37% 18/49 [00:03<00:05,  6.13it/s]\u001b[A\n",
            "Iteration:  39% 19/49 [00:03<00:04,  6.13it/s]\u001b[A\n",
            "Iteration:  41% 20/49 [00:03<00:04,  6.13it/s]\u001b[A\n",
            "Iteration:  43% 21/49 [00:03<00:04,  6.10it/s]\u001b[A\n",
            "Iteration:  45% 22/49 [00:03<00:04,  6.07it/s]\u001b[A\n",
            "Iteration:  47% 23/49 [00:03<00:04,  6.07it/s]\u001b[A\n",
            "Iteration:  49% 24/49 [00:04<00:04,  6.07it/s]\u001b[A\n",
            "Iteration:  51% 25/49 [00:04<00:03,  6.07it/s]\u001b[A\n",
            "Iteration:  53% 26/49 [00:04<00:03,  6.09it/s]\u001b[A\n",
            "Iteration:  55% 27/49 [00:04<00:03,  6.09it/s]\u001b[A\n",
            "Iteration:  57% 28/49 [00:04<00:03,  6.10it/s]\u001b[A\n",
            "Iteration:  59% 29/49 [00:04<00:03,  6.11it/s]\u001b[A\n",
            "Iteration:  61% 30/49 [00:05<00:03,  6.12it/s]\u001b[A\n",
            "Iteration:  63% 31/49 [00:05<00:02,  6.12it/s]\u001b[A\n",
            "Iteration:  65% 32/49 [00:05<00:02,  6.08it/s]\u001b[A\n",
            "Iteration:  67% 33/49 [00:05<00:02,  6.09it/s]\u001b[A\n",
            "Iteration:  69% 34/49 [00:05<00:02,  6.07it/s]\u001b[A\n",
            "Iteration:  71% 35/49 [00:05<00:02,  6.07it/s]\u001b[A\n",
            "Iteration:  73% 36/49 [00:06<00:02,  6.08it/s]\u001b[A\n",
            "Iteration:  76% 37/49 [00:06<00:01,  6.07it/s]\u001b[A\n",
            "Iteration:  78% 38/49 [00:06<00:01,  6.08it/s]\u001b[A\n",
            "Iteration:  80% 39/49 [00:06<00:01,  6.05it/s]\u001b[A\n",
            "Iteration:  82% 40/49 [00:06<00:01,  6.07it/s]\u001b[A\n",
            "Iteration:  84% 41/49 [00:06<00:01,  6.07it/s]\u001b[A\n",
            "Iteration:  86% 42/49 [00:07<00:01,  6.08it/s]\u001b[A\n",
            "Iteration:  88% 43/49 [00:07<00:00,  6.07it/s]\u001b[A\n",
            "Iteration:  90% 44/49 [00:07<00:00,  6.06it/s]\u001b[A\n",
            "Iteration:  92% 45/49 [00:07<00:00,  6.05it/s]\u001b[A\n",
            "Iteration:  94% 46/49 [00:07<00:00,  6.05it/s]\u001b[A\n",
            "Iteration:  96% 47/49 [00:07<00:00,  6.01it/s]\u001b[A\n",
            "Iteration:  98% 48/49 [00:08<00:00,  6.02it/s]\u001b[A\n",
            "Iteration: 100% 49/49 [00:08<00:00,  6.00it/s]\n",
            "Epoch:  50% 1/2 [00:08<00:08,  8.17s/it]\n",
            "Iteration:   0% 0/49 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   2% 1/49 [00:00<00:07,  6.36it/s]\u001b[A\n",
            "Iteration:   4% 2/49 [00:00<00:07,  6.23it/s]\u001b[A\n",
            "Iteration:   6% 3/49 [00:00<00:07,  6.17it/s]\u001b[A\n",
            "Iteration:   8% 4/49 [00:00<00:07,  6.11it/s]\u001b[A\n",
            "Iteration:  10% 5/49 [00:00<00:07,  6.09it/s]\u001b[A\n",
            "Iteration:  12% 6/49 [00:00<00:07,  6.06it/s]\u001b[A\n",
            "Iteration:  14% 7/49 [00:01<00:06,  6.08it/s]\u001b[A\n",
            "Iteration:  16% 8/49 [00:01<00:06,  6.08it/s]\u001b[A\n",
            "Iteration:  18% 9/49 [00:01<00:06,  6.08it/s]\u001b[A\n",
            "Iteration:  20% 10/49 [00:01<00:06,  6.07it/s]\u001b[A\n",
            "Iteration:  22% 11/49 [00:01<00:06,  6.06it/s]\u001b[A\n",
            "Iteration:  24% 12/49 [00:01<00:06,  6.05it/s]\u001b[A\n",
            "Iteration:  27% 13/49 [00:02<00:05,  6.04it/s]\u001b[A\n",
            "Iteration:  29% 14/49 [00:02<00:05,  6.05it/s]\u001b[A\n",
            "Iteration:  31% 15/49 [00:02<00:05,  6.03it/s]\u001b[A\n",
            "Iteration:  33% 16/49 [00:02<00:05,  6.00it/s]\u001b[A\n",
            "Iteration:  35% 17/49 [00:02<00:05,  5.99it/s]\u001b[A\n",
            "Iteration:  37% 18/49 [00:02<00:05,  6.01it/s]\u001b[A\n",
            "Iteration:  39% 19/49 [00:03<00:04,  6.01it/s]\u001b[A\n",
            "Iteration:  41% 20/49 [00:03<00:04,  6.01it/s]\u001b[A\n",
            "Iteration:  43% 21/49 [00:03<00:04,  6.01it/s]\u001b[A\n",
            "Iteration:  45% 22/49 [00:03<00:04,  6.01it/s]\u001b[A\n",
            "Iteration:  47% 23/49 [00:03<00:04,  6.03it/s]\u001b[A\n",
            "Iteration:  49% 24/49 [00:03<00:04,  6.03it/s]\u001b[A\n",
            "Iteration:  51% 25/49 [00:04<00:03,  6.03it/s]\u001b[A\n",
            "Iteration:  53% 26/49 [00:04<00:03,  5.99it/s]\u001b[A\n",
            "Iteration:  55% 27/49 [00:04<00:03,  5.99it/s]\u001b[A\n",
            "Iteration:  57% 28/49 [00:04<00:03,  5.96it/s]\u001b[A\n",
            "Iteration:  59% 29/49 [00:04<00:03,  5.98it/s]\u001b[A\n",
            "Iteration:  61% 30/49 [00:04<00:03,  6.01it/s]\u001b[A\n",
            "Iteration:  63% 31/49 [00:05<00:03,  5.99it/s]\u001b[A\n",
            "Iteration:  65% 32/49 [00:05<00:02,  5.96it/s]\u001b[A\n",
            "Iteration:  67% 33/49 [00:05<00:02,  5.99it/s]\u001b[A\n",
            "Iteration:  69% 34/49 [00:05<00:02,  6.00it/s]\u001b[A\n",
            "Iteration:  71% 35/49 [00:05<00:02,  5.98it/s]\u001b[A\n",
            "Iteration:  73% 36/49 [00:05<00:02,  5.98it/s]\u001b[A\n",
            "Iteration:  76% 37/49 [00:06<00:02,  5.94it/s]\u001b[A\n",
            "Iteration:  78% 38/49 [00:06<00:01,  5.96it/s]\u001b[A\n",
            "Iteration:  80% 39/49 [00:06<00:01,  6.00it/s]\u001b[A\n",
            "Iteration:  82% 40/49 [00:06<00:01,  6.00it/s]\u001b[A\n",
            "Iteration:  84% 41/49 [00:06<00:01,  5.99it/s]\u001b[A\n",
            "Iteration:  86% 42/49 [00:06<00:01,  5.99it/s]\u001b[A\n",
            "Iteration:  88% 43/49 [00:07<00:01,  5.99it/s]\u001b[A\n",
            "Iteration:  90% 44/49 [00:07<00:00,  5.99it/s]\u001b[A\n",
            "Iteration:  92% 45/49 [00:07<00:00,  5.98it/s]\u001b[A\n",
            "Iteration:  94% 46/49 [00:07<00:00,  5.97it/s]\u001b[A\n",
            "Iteration:  96% 47/49 [00:07<00:00,  5.99it/s]\u001b[A\n",
            "Iteration:  98% 48/49 [00:07<00:00,  5.99it/s]\u001b[A\n",
            "Iteration: 100% 49/49 [00:08<00:00,  6.01it/s]\n",
            "Epoch: 100% 2/2 [00:16<00:00,  8.16s/it]\n",
            "04/07/2021 16:55:02 - INFO - __main__ -    global_step = 98, average loss = 3.2220906371973\n",
            "04/07/2021 16:55:02 - INFO - __main__ -   Saving model checkpoint to models\n",
            "04/07/2021 16:55:02 - INFO - transformers.configuration_utils -   Configuration saved in models/config.json\n",
            "04/07/2021 16:55:03 - INFO - transformers.modeling_utils -   Model weights saved in models/pytorch_model.bin\n",
            "04/07/2021 16:55:03 - INFO - transformers.configuration_utils -   loading configuration file models/config.json\n",
            "04/07/2021 16:55:03 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
            "  \"_num_labels\": 2,\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bad_words_ids\": null,\n",
            "  \"bos_token_id\": null,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": null,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "04/07/2021 16:55:03 - INFO - transformers.modeling_utils -   loading weights file models/pytorch_model.bin\n",
            "04/07/2021 16:55:06 - INFO - transformers.configuration_utils -   loading configuration file models/config.json\n",
            "04/07/2021 16:55:06 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
            "  \"_num_labels\": 2,\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bad_words_ids\": null,\n",
            "  \"bos_token_id\": null,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": null,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "04/07/2021 16:55:06 - INFO - transformers.tokenization_utils -   Model name 'models' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'models' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "04/07/2021 16:55:06 - INFO - transformers.tokenization_utils -   Didn't find file models/added_tokens.json. We won't load it.\n",
            "04/07/2021 16:55:06 - INFO - transformers.tokenization_utils -   loading file models/vocab.txt\n",
            "04/07/2021 16:55:06 - INFO - transformers.tokenization_utils -   loading file None\n",
            "04/07/2021 16:55:06 - INFO - transformers.tokenization_utils -   loading file models/special_tokens_map.json\n",
            "04/07/2021 16:55:06 - INFO - transformers.tokenization_utils -   loading file models/tokenizer_config.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WkcpbOeGQU7j",
        "outputId": "88ed51ae-495e-4ef6-cb32-fe7b41c3654e"
      },
      "source": [
        "!python3 'generate_transformers.py' \\\n",
        "    --model_type=bert \\\n",
        "    --model_name_or_path=models \\\n",
        "    --k=5 \\\n",
        "    --p=0.95 \\\n",
        "    --length=30"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "04/07/2021 17:23:53 - INFO - __main__ -   Namespace(device=device(type='cuda'), k=5, length=30, model_name_or_path='models', model_type='bert', n_gpu=1, no_cuda=False, num_return_sequences=1, p=0.95, padding_text='', prompt='', repetition_penalty=1.0, seed=42, stop_token='</s>', temperature=1.0, xlm_language='')\n",
            "Context >>> import pandas\n",
            "Model:\n",
            "2021-04-07 17:24:03.141919: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "import pandassssssssssssssssssssssssssssssssssssssssssssssssssssssss\n",
            "Context >>> import pandas as \n",
            "Model:\n",
            "import pandas as  as or or or or or or or or or or or or,,,,,,,,,,,,,,,,\n",
            "Context >>> Traceback (most recent call last):\n",
            "  File \"generate_transformers.py\", line 271, in <module>\n",
            "    main()\n",
            "  File \"generate_transformers.py\", line 216, in main\n",
            "    prompt_text = args.prompt if args.prompt else input(\"Context >>> \")\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}